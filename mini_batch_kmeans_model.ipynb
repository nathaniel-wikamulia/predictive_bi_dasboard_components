{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "_R2cYL2yFU4Q",
        "D3HHgEcM9ILu",
        "XSkse_9C9OdZ",
        "RsCY5-9c9SwL",
        "c9_SOCaMCU1j",
        "IJ3kXStWCbna",
        "W80RXawuCCUW",
        "4sX2le8ZCPHL",
        "NKLF20phEYin",
        "eSQZwELWHX1k",
        "orSmmXY0EnAo"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Mount Google Drive"
      ],
      "metadata": {
        "id": "_R2cYL2yFU4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "\n",
        "#Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XnV6hX4FRLs",
        "outputId": "a726cdbf-728b-498c-db45-e7b9807e7e75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import Library"
      ],
      "metadata": {
        "id": "D3HHgEcM9ILu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXy1Nx4U9CS5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy\n",
        "import datetime\n",
        "import seaborn as sb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "from datetime import timedelta\n",
        "\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "\n",
        "!cp drive/MyDrive/base/base.py .\n",
        "import base\n",
        "\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.metrics import davies_bouldin_score\n",
        "from sklearn.metrics import calinski_harabasz_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Initialize The CSV Location"
      ],
      "metadata": {
        "id": "XSkse_9C9OdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize The CSV Directory\n",
        "dataset_directory = \"drive/MyDrive/Dataset Bakery XYZ/\"\n",
        "\n",
        "#Initialize The CSV File Names\n",
        "directory_area = dataset_directory + \"AREA.csv\"\n",
        "directory_customer = dataset_directory + \"CUST.csv\"\n",
        "directory_inventory = dataset_directory + \"INVENTORY.csv\"\n",
        "directory_sales_header = dataset_directory + \"SALESHEADER.csv\"\n",
        "directory_sales_detail = dataset_directory + \"SALESDETAIL.csv\""
      ],
      "metadata": {
        "id": "lUxuCb049MZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Preprocessing"
      ],
      "metadata": {
        "id": "RsCY5-9c9SwL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Perform Data Preprocessing on All Tables"
      ],
      "metadata": {
        "id": "c9_SOCaMCU1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Preprocessing The \"Area\" Table\n",
        "area = pd.read_csv(directory_area, \";\")\n",
        "\n",
        "area.drop(['UPDDATE', 'UPDTIME'], inplace=True, axis=1)\n",
        "\n",
        "desc = area['DESC'].unique()\n",
        "desc_dict = dict(zip(desc, range(len(desc))))\n",
        "area = area.applymap(lambda s: desc_dict.get(s) if s in desc_dict else s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKB4Sb1n9YHp",
        "outputId": "89164d93-746f-4b38-9294-ec1b7379e6ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:3326: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Preprocessing The \"Customer\" Table\n",
        "customer = pd.read_csv(directory_customer, \";\")\n",
        "\n",
        "customer.drop(['UPDDATE', 'UPDTIME'], inplace=True, axis=1)\n",
        "\n",
        "salestypes = customer['SALTYPE'].unique()\n",
        "salestypes_dict = dict(zip(salestypes, range(len(salestypes))))\n",
        "customer = customer.applymap(lambda s: salestypes_dict.get(s) if s in salestypes_dict else s)\n",
        "\n",
        "(unique, counts) = numpy.unique(customer['INACTIVE'], return_counts=True)\n",
        "frequencies = numpy.asarray((unique, counts)).T\n",
        "customer.drop(customer.loc[customer['INACTIVE']==2].index, inplace=True)\n",
        "\n",
        "inactive = customer['INACTIVE'].unique()\n",
        "inactive_dict = dict(zip(inactive, range(len(inactive))))\n",
        "customer = customer.applymap(lambda s: inactive_dict.get(s) if s in inactive_dict else s)"
      ],
      "metadata": {
        "id": "GK2xhZp2AkFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Preprocessing The \"Inventory\" Table\n",
        "inventory = pd.read_csv(directory_inventory, \";\")\n",
        "\n",
        "inventory.drop(['UPDDATE', 'UPDTIME'], inplace=True, axis=1)\n",
        "\n",
        "inventory['SPRICE'] = inventory['SPRICE'].str.replace(',00', '')\n",
        "inventory['SPRICE'] = pd.to_numeric(inventory['SPRICE'],errors = 'coerce')\n",
        "\n",
        "inventory['UCOST'] = inventory['UCOST'].str.replace(',00', '')\n",
        "inventory['UCOST'] = pd.to_numeric(inventory['UCOST'],errors = 'coerce')\n",
        "\n",
        "inventory['WEIGHT'] = inventory['WEIGHT'].str.replace(',00', '')\n",
        "inventory['WEIGHT'] = pd.to_numeric(inventory['WEIGHT'],errors = 'coerce')"
      ],
      "metadata": {
        "id": "NgekcqEKAmPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Preprocessing The \"Sales Detail\" Table\n",
        "salesDetail = pd.read_csv(directory_sales_detail, \";\")\n",
        "\n",
        "salesDetail.drop(['UPDDATE', 'UPDTIME'], inplace=True, axis=1)\n",
        "\n",
        "salesDetail['UPRICE'] = salesDetail['UPRICE'].str.replace(',00', '')\n",
        "salesDetail['UPRICE'] = pd.to_numeric(salesDetail['UPRICE'],errors = 'coerce')\n",
        "\n",
        "salesDetail['UCOST'] = salesDetail['UCOST'].str.replace(',00', '')\n",
        "salesDetail['UCOST'] = pd.to_numeric(salesDetail['UCOST'],errors = 'coerce')\n",
        "\n",
        "salesDetail['AMOUNT'] = salesDetail['AMOUNT'].str.replace(',00', '')\n",
        "salesDetail['AMOUNT'] = pd.to_numeric(salesDetail['AMOUNT'],errors = 'coerce')\n",
        "\n",
        "salesDetail['DISCAMT'] = salesDetail['DISCAMT'].str.replace(',00', '')\n",
        "salesDetail['DISCAMT'] = pd.to_numeric(salesDetail['DISCAMT'],errors = 'coerce')\n"
      ],
      "metadata": {
        "id": "PatGNIhdAoAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Preprocessing The \"Sales Header\" Table\n",
        "salesHeader = pd.read_csv(directory_sales_header, \";\")\n",
        "\n",
        "salesHeader.drop(['UPDDATE', 'UPDTIME'], inplace=True, axis=1)\n",
        "\n",
        "salesHeader['TOTAL'] = salesHeader['TOTAL'].str.replace(',00', '')\n",
        "salesHeader['TOTAL'] = pd.to_numeric(salesHeader['TOTAL'],errors = 'coerce')\n",
        "\n",
        "STYPE = salesHeader['STYPE'].unique()\n",
        "STYPE_dict = dict(zip(STYPE, range(len(STYPE))))\n",
        "salesHeader = salesHeader.applymap(lambda s: STYPE_dict.get(s) if s in STYPE_dict else s)\n",
        "\n",
        "salesHeader['TRDATE'] = pd.to_datetime(salesHeader['TRDATE'])"
      ],
      "metadata": {
        "id": "YGG0lgBMBNAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Performing JOIN on All Tables"
      ],
      "metadata": {
        "id": "IJ3kXStWCbna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#JOIN The \"Customer\" Table and \"Area\" Table \n",
        "area.rename(columns={\"CODE\": \"AreaID\"}, inplace=True)\n",
        "customer.rename(columns={\"AREACD\": \"AreaID\", \"CODE\": \"CustomerID\"}, inplace=True)\n",
        "\n",
        "(unique, counts) = numpy.unique(customer['AreaID'], return_counts=True)\n",
        "frequencies = numpy.asarray((unique, counts)).T\n",
        "\n",
        "testJoin = customer.merge(area,\n",
        "                    on=['AreaID'],\n",
        "                    how=\"outer\"\n",
        "                    )"
      ],
      "metadata": {
        "id": "5HAGbNh7BWcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#JOIN The \"Sales Header\" Table with The \"Customer and Area\" Table\n",
        "salesHeader.rename(columns={\"CUSTCODE\": \"CustomerID\"}, inplace=True)\n",
        "\n",
        "(unique, counts) = numpy.unique(salesHeader['CustomerID'], return_counts=True)\n",
        "frequencies = numpy.asarray((unique, counts)).T\n",
        "\n",
        "testJoin.rename(columns={\"CODE\": \"CustomerID\"}, inplace=True)\n",
        "\n",
        "testMultiJoin = testJoin.merge(salesHeader,\n",
        "                               on=['CustomerID'],\n",
        "                               how=\"right\"\n",
        "                               )"
      ],
      "metadata": {
        "id": "b0NPjV4aBaVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#JOIN The \"Sales Detail\" Table with The \"Sales Header, Customer, and Area\" Table\n",
        "salesDetail.rename(columns={\"TRNO\": \"SalesHeaderID\"}, inplace=True)\n",
        "testMultiJoin.rename(columns={\"TRNO\": \"SalesHeaderID\"}, inplace=True)\n",
        "\n",
        "saleHeaderSalesDetailJoin = testMultiJoin.merge(salesDetail,\n",
        "                                                on=['SalesHeaderID'],\n",
        "                                                how=\"right\"\n",
        "                                                )"
      ],
      "metadata": {
        "id": "2d7IfAoCBd4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#JOIN The \"Inventory\" Table with The \"Sales Detail, Sales Header, Customer, and Area\" Table\n",
        "saleHeaderSalesDetailJoin.rename(columns={\"ITEMNO\": \"ProductID\"}, inplace=True)\n",
        "inventory.rename(columns={\"ITEMNO\": \"ProductID\"}, inplace=True)\n",
        "\n",
        "MergedData= saleHeaderSalesDetailJoin.merge(inventory,\n",
        "                                      on=['ProductID'],\n",
        "                                      how=\"left\"\n",
        "                               )"
      ],
      "metadata": {
        "id": "YY2qBF8GBffc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating The Recency, Frequency, and Monetary value (RFM) Table"
      ],
      "metadata": {
        "id": "W80RXawuCCUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing Unused Columns\n",
        "MergedData.drop(['STYPE', 'SALTYPE', 'ARBAL', 'LINENO','DISCAMT','ITEMNAME','WEIGHT','UCOST_y','UCOST_x','INACTIVE', 'SPRICE','DESC','SALPERSON','TOTAL','ProductID','UPRICE','AreaID', 'QTY', 'QTYRET', 'CUSTNAME'], inplace=True, axis=1)"
      ],
      "metadata": {
        "id": "ldeC4-dICD07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating the Recency, Frequency, and Monetary Value of Each Customer\n",
        "max_date = MergedData['TRDATE'].max() + timedelta(days=1)\n",
        "\n",
        "rfm_data = MergedData.groupby(['CustomerID']).agg({\n",
        "    'TRDATE': lambda x: (max_date - x.max()).days,\n",
        "    'SalesHeaderID': 'count',\n",
        "    'AMOUNT': 'sum' \n",
        "})\n",
        "\n",
        "rfm_data.rename(columns = {'TRDATE':'recency', 'SalesHeaderID':'frequency','AMOUNT':'monetary_value'}, inplace = True)\n",
        "rfm_data.index.names = ['customer_id']"
      ],
      "metadata": {
        "id": "0MPja7nDCIF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Changing The RFM Value To Scale 1-6"
      ],
      "metadata": {
        "id": "4sX2le8ZCPHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a \"set_quantile\" Function\n",
        "\n",
        "#Purpose:\n",
        "#The \"set_quantile\" function splits the input data into several equal length parts and then number each part with the appropriate number.\n",
        "#In this case it splits the data into 6 sections and then number the sections into a number from 1-6.\n",
        "\n",
        "#Input Parameter:\n",
        "#The function that accepts two parameter as an input: \n",
        "#   1. \"rfm_dataframe_without_quantile\": The dataframe that will be used by the function.\n",
        "#   2. \"number_of_quantile\": The maximum number that can be assigned to a data.\n",
        "\n",
        "#Return Value:\n",
        "#The function returns one item:\n",
        "#   1. \"rfm_dataframe_with_quantile\": The dataframe that has been converted in quantile form.\n",
        "\n",
        "def set_quantile(rfm_dataframe_without_quantile , number_of_quantile):\n",
        "  r_labels = range(number_of_quantile, 0, -1) \n",
        "  r_groups = pd.qcut(rfm_dataframe_without_quantile['recency'], q = number_of_quantile, labels = r_labels)\n",
        "  f_labels = range(1, number_of_quantile + 1)\n",
        "  f_groups = pd.qcut(rfm_dataframe_without_quantile['frequency'], q = number_of_quantile, labels = f_labels)\n",
        "  m_labels = range(1, number_of_quantile + 1)\n",
        "  m_groups = pd.qcut(rfm_dataframe_without_quantile['monetary_value'], q = number_of_quantile, labels = m_labels)\n",
        "  rfm_dataframe_with_quantile = rfm_dataframe_without_quantile.assign(R = r_groups.values, F = f_groups.values, M = m_groups.values)\n",
        "  rfm_dataframe_with_quantile[['R', 'F','M']] = rfm_dataframe_with_quantile[['R', 'F', 'M']].apply(pd.to_numeric)\n",
        "  rfm_dataframe_with_quantile.drop(['recency','frequency','monetary_value'], inplace=True, axis=1)\n",
        "  return rfm_dataframe_with_quantile"
      ],
      "metadata": {
        "id": "aj0zbpM4ET8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calling The \"set_quantile\" Function to Divide the Data into Number 1-6.\n",
        "rfm_quantile_dataframe = set_quantile(rfm_data, 6)"
      ],
      "metadata": {
        "id": "KuqG1Da_EVnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating The Mini Batch K-Means Model"
      ],
      "metadata": {
        "id": "NKLF20phEYin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating The \"mini_batch_kmeans\" Function\n",
        "\n",
        "#Purpose:\n",
        "#The \"mini_batch_kmeans\" function creates a Mini Batch K-Means model and assigns each customer to a cluster based on the customer's RFM value.\n",
        "\n",
        "#Input Parameter:\n",
        "#The function that accepts two parameter as an input: \n",
        "#   1. \"df_rfm\": The dataframe that will be used as training dataset.\n",
        "#   2. \"clusters_number\": The number of clusters to be created.\n",
        "\n",
        "#Return Value:\n",
        "#The function returns two item:\n",
        "#   1. \"df_new\": A dataframe containing the customer's name, the RFM value of the customer, and the categorization of the customer.\n",
        "#   2. \"y_pred\": A list containing the categorization of each customer.\n",
        "\n",
        "def mini_batch_kmeans(df_rfm, clusters_number=3):\n",
        "  model_mbkm = MiniBatchKMeans(n_clusters = clusters_number,\n",
        "                               random_state = 1, \n",
        "                               max_iter=30, \n",
        "                               init='k-means++',\n",
        "                               tol=0,\n",
        "                               max_no_improvement=10,\n",
        "                               init_size=140,\n",
        "                               n_init=3,\n",
        "                               reassignment_ratio = 0.01,\n",
        "                               batch_size=1536\n",
        "                               )\n",
        "  model_mbkm.fit(df_rfm)\n",
        "  y_pred = model_mbkm.predict(df_rfm)\n",
        "  mbkm_cluster_labels = model_mbkm.labels_\n",
        "  df_new = df_rfm.assign(Cluster = mbkm_cluster_labels)\n",
        "  return df_new, y_pred"
      ],
      "metadata": {
        "id": "KzTWPCG5EY9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize The Number of Clusters\n",
        "number_of_clusters = 6"
      ],
      "metadata": {
        "id": "nSejKFBeEuyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Execute The \"mini_batch_kmeans\" Function\n",
        "rfm_df_post_mbkm, mbkm_prediction = mini_batch_kmeans(rfm_quantile_dataframe, number_of_clusters)"
      ],
      "metadata": {
        "id": "wlO6GgeeEhMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Evaluation"
      ],
      "metadata": {
        "id": "eSQZwELWHX1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating The \"get_cluster_list\" Function\n",
        "\n",
        "#Purpose:\n",
        "#The \"get_cluster_list\" function  groups data points that have the same category into a list.\n",
        "\n",
        "#Input Parameter:\n",
        "#The function that accepts two parameter as an input: \n",
        "#   1. \"rfm_df_post_model\": The dataframe that will be used as the input dataset.\n",
        "#   2. \"number_of_clusters\": The number of clusters created.\n",
        "\n",
        "#Return Value:\n",
        "#The function returns two item:\n",
        "#   1. \"cluster_list\": A list containing each data point that is grouped by category.\n",
        "\n",
        "def get_cluster_list(rfm_df_post_model, number_of_clusters):\n",
        "  clus_i_temp=[]\n",
        "  cluster_list = []\n",
        "  for i in range(number_of_clusters):\n",
        "    clus_i_temp = rfm_df_post_model.loc[rfm_df_post_model.Cluster == i]\n",
        "    cluster_list.append(clus_i_temp.values)\n",
        "  return cluster_list"
      ],
      "metadata": {
        "id": "9xtFjyui8Idy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dunn Index\n",
        "#The higher the better\n",
        "\n",
        "cluster_list_mbkm = get_cluster_list(rfm_df_post_mbkm, number_of_clusters)\n",
        "print(f\"Dunn Index: {base.dunn(cluster_list_mbkm)}\")"
      ],
      "metadata": {
        "id": "-Yyp8q3d6MAA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69234da4-7a46-4f6b-b243-5c5c04a53609"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dunn Index: 0.4264014327112209\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Silhouette Score\n",
        "#The higher the better\n",
        "\n",
        "print(f\"Silhouette Score: {silhouette_score(rfm_quantile_dataframe, mbkm_prediction)}\")"
      ],
      "metadata": {
        "id": "I3I_lQ4Q6Odl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "909e322a-3ace-4728-de70-9e57a0720275"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Silhouette Score: 0.42822601036824676\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Davies Bouldin Index\n",
        "#The lower the better\n",
        "\n",
        "print(f\"Davies Bouldin Index: {davies_bouldin_score(rfm_quantile_dataframe, mbkm_prediction)}\")"
      ],
      "metadata": {
        "id": "EA8EfvZ4HXIQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb1181b6-14e9-44ff-bb18-c4fc26ef2068"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Davies Bouldin Index: 0.8724920185820473\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Converting The Dataframe to CSV"
      ],
      "metadata": {
        "id": "orSmmXY0EnAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Merge the dataframes containing the clustering results and the RFM values that have not been converted into 1-6.\n",
        "rfm_combined = rfm_df_post_mbkm.merge(rfm_data,\n",
        "                    on=['customer_id'],\n",
        "                    how=\"inner\"\n",
        "                    )"
      ],
      "metadata": {
        "id": "4wct0P32Elic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting the previously combined dataframe results into CSV and then download the CSV.\n",
        "rfm_combined.to_csv(\"rfm_per_customer.csv\")\n",
        "files.download(\"rfm_per_customer.csv\")"
      ],
      "metadata": {
        "id": "NBMXhJGEErta"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}